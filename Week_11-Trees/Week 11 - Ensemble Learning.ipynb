{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a132e02a",
   "metadata": {},
   "source": [
    "# Week 11: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d3983",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "* Take a random sample with replacement\n",
    "* Repeat \n",
    "* Models are trained independently\n",
    "* Take average of models\n",
    "* Random Forest is an extension of Bagging\n",
    "\n",
    "https://www.ibm.com/cloud/learn/bagging\n",
    "\n",
    "### Boosting\n",
    "\n",
    "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "\n",
    "* Great for reducing bias and variance\n",
    "* Train a weak learner such as a classifier that performs slightly better than random guessing\n",
    "* Find misclassifications and train another weak learner that focuses on the misclassifications\n",
    "* Use weights to reward, or emphasize, the weak learner (large weights) and penalize the strong learner (small weights) \n",
    "* Data is also weighted, misclassified data are weighted as more important than correctly classified data\n",
    "* Each weak learner learns from the previous learner\n",
    "* Continue process until desired output is reached and then aggregate all the learners\n",
    "\n",
    "<img src='https://miro.medium.com/max/1200/1*zTgGBTQIMlASWm5QuS2UpA.jpeg' alt='bagging vs boosting' />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
