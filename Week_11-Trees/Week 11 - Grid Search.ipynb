{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23afe35",
   "metadata": {},
   "source": [
    "# Week 11: Grid Search\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned... The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hyperparameter_optimization\n",
    "\n",
    "**The following code will take some time because it has to create a different model for each of the values we provide**\n",
    "\n",
    "The hyperparameters have been limited so that the code runs faster. More values would give us a better idea of how accurate our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6ec430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy 0.9371212121212119\n",
      "best parameters {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 3, 'max_leaf_nodes': 7, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# example of randomforest classifier, grid search, and cross validation (code example for assignment)\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.25, random_state=42)\n",
    "\n",
    "hyperparameters = {\n",
    "            'n_estimators': [50, 200],\n",
    "            'criterion': ['entropy', 'gini'],\n",
    "            'max_depth': [3, 4],\n",
    "            'max_leaf_nodes': [7, 9],\n",
    "            'bootstrap': [True, False]\n",
    "            }\n",
    "\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(),\n",
    "                           param_grid = hyperparameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters', best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d92059",
   "metadata": {},
   "source": [
    "### Attribute Selection Measures\n",
    "\n",
    "* Gini Index/Impurity: Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "* Entropy: In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\", and is sometimes called Shannon entropy in his honour. https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "* Information Gain: Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value \"represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node\". \n",
    "* https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\n",
    "* https://datascience.foundation/sciencewhitepaper/understanding-decision-trees-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ec79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(criterion='entropy', max_depth=3, max_leaf_nodes=7,\n",
      "                       n_estimators=50, random_state=42)>\n"
     ]
    }
   ],
   "source": [
    "# example of classification using random forests\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df.drop('species', axis=1), iris_df['species'], test_size=0.20)\n",
    "\n",
    "model = RandomForestClassifier(random_state = 42).set_params(**best_parameters) # * args, ** kwargs\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(model.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53551306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
